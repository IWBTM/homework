



## 14.7 

These proofs are tricky for those not accustomed to manipulating probability expres

sions, and students may require some hints.

**a.** There are several ways to prove this. Probably the simplest is to work directly from the

global semantics. First, we rewrite the required probability in terms of the full joint:

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.25.56.png" alt="截屏2020-11-10 下午11.25.56" style="zoom:50%;" />

Now, all terms in the product in the denominator that do not contain xi can be moved

outside the summation, and then cancel with the corresponding terms in the numerator.

This just leaves us with the terms that do mention $x_i$

 those in which $X_i$ is a child or a parent. Hence $P(x_i|x_1,... ,x_{i-1},x_{i+1},... ,x_n) $ is equal to 

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.28.11.png" alt="截屏2020-11-10 下午11.28.11" style="zoom:50%;" />

Now, by reversing the argument in part (b), we obtain the desired result.

**b.**

This is a relatively straightforward application of Bayes’ rule. Let **Y** = Y1,... ,yℓ be the

children of Xi and let **Z**j be the parents of Yj other than Xi . Then we have

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.29.48.png" alt="截屏2020-11-10 下午11.29.48" style="zoom:50%;" />

where the derivation of the third line from the second relies on the fact that a node is

independent of its nondescendants given its children.





## 14.14

**a.** (2),(3) can be asserted

**b.** $p(b,i,\not m,g,j)=p(b)*p(\not m)p(i|b,\not m)*p(g|b,i,\not m)*p(j|g,b,i,\not m)= 0.2916$

**c**. $p(j)= p(g|b,i,m)*p(j|g)=0.81$

**e.** A pardon is unnecessary if the person is not indicted or not found guilty; so I and G

are parents of P. One could also add B and M as parents of P, since a pardon is more

likely if the person is actually innocent and if the prosecutor is politically motivated.

(There are other causes of Pardon, such as LargeDonationToPresidentsParty,

but such variables are not currently in the model.) The pardon (presumably) is a get

out-of-jail-free card, so P is a parent of J.





## 14.18

**a.**  There are two uninstantiated Boolean variables (Cloudy and Rain) and therefore four

possible states.

**b.**  First, we compute the sampling distribution for each variable, conditioned on its Markov

blanket

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.31.20.png" alt="截屏2020-11-10 下午11.31.20" style="zoom:50%;" />

Strictly speaking, the transition matrix is only well-defifined for the variant of MCMC in

which the variable to be sampled is chosen randomly. (In the variant where the variables

are chosen in a fifixed order, the transition probabilities depend on where we are in the

ordering.) Now consider the transition matrix



Entries on the diagonal correspond to self-loops. Such transitions can occur by

sampling *either* variable. For example,

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.32.07.png" alt="截屏2020-11-10 下午11.32.07" style="zoom:50%;" />

Entries where one variable is changed must sample that variable. For example,

<img src="/Users/tjc/Desktop/截屏2020-11-10 下午11.33.38.png" alt="截屏2020-11-10 下午11.33.38" style="zoom:50%;" />

 Entries where both variables change cannot occur. For example,

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.34.09.png" alt="截屏2020-11-10 下午11.34.09" style="zoom:50%;" />

This gives us the following transition matrix, where the transition is from the state given

by the row label to the state given by the column label:

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午11.34.29.png" alt="截屏2020-11-10 下午11.34.29" style="zoom:50%;" />

**c.** $Q^2$ represents the probability of going from each state to each state in two steps.

**d.**  $Q^n$ (as n → ∞) represents the long-term probability of being in each state starting in

each state; for ergodic **Q** these probabilities are independent of the starting state, so

every row of **Q** is the same and represents the posterior distribution over states given

the evidence.

**e.** We can produce very large powers of **Q** with very few matrix multiplications. For

example, we can get $Q^2$ with one multiplication, $Q^4$ with two, and $Q^2$ with k. Unfortunately, in a network with n Boolean variables, the matrix is of size 2n × 2n , so each multiplication takes $O(2^{3n})$ operations.

## 14.21

**a**. we can create class **Team**, and create three instance A,B,C. Then we can create class **match**, with three instance AB,BC,CD.  Each team has a quality Q, the probabilty of X win Y increases with Q(X)-Q(Y)



**c**.The exact result will depend on the probabilities used in the model. With any prior on quality that is the same across all teams, we expect that the posterior over BC.Outcome will show that C is more likely to win than B. 

**d.** The inference cost in such a model will be $O(2^n)$ because all the team qualities become coupled.

**e.** MCMC appears to do well on this problem, provided the probabilities are not too skewed. Our results show scaling behavior that is roughly linear in the number of teams, although we did not investigate very large n.





**20.9**

**a.**  The probability of a positive example is $π$ and of a negative example is $(1-π)$, and the

data are independent, so the probability of the data is $π^p(1-π)^n$



**b.**  We have $L = p log π + n log(1-π);$ if the derivative is zero, we have

​	<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-05 下午11.39.50.png" alt="截屏2020-11-05 下午11.39.50" style="zoom:50%;" />

  so the ML value is $π = p/(p + n)$, i.e., the proportion of positive examples in the data.

**c.**  This is the “naive Bayes” probability model.

​	<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午10.35.30.png" alt="截屏2020-11-10 下午10.35.30" style="zoom:50%;" />



**d.** 

<img src="/Users/tjc/Library/Application Support/typora-user-images/截屏2020-11-10 下午10.36.29.png" alt="截屏2020-11-10 下午10.36.29" style="zoom:50%;" />

*f.**  In the data set we have$ p = 2, n = 2, p^+_i = 1, n^+_i = 1, p_i = 1, n_i = 1$. From our formulæ, we obtain π = α1 = α2 = β1 = β2 = 0.5. 

**g.** Each example is predicted to be positive with probability 0.5.

